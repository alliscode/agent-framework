// <auto-generated/>

#nullable disable

using System;
using System.Collections.Generic;

namespace AzureAIAgents.Models
{
    /// <summary> The CreateResponse. </summary>
    public partial class CreateResponse
    {
        /// <summary>
        /// Keeps track of any properties unknown to the library.
        /// <para>
        /// To assign an object to the value of this property use <see cref="BinaryData.FromObjectAsJson{T}(T, System.Text.Json.JsonSerializerOptions?)"/>.
        /// </para>
        /// <para>
        /// To assign an already formatted json string to this property use <see cref="BinaryData.FromString(string)"/>.
        /// </para>
        /// <para>
        /// Examples:
        /// <list type="bullet">
        /// <item>
        /// <term>BinaryData.FromObjectAsJson("foo")</term>
        /// <description>Creates a payload of "foo".</description>
        /// </item>
        /// <item>
        /// <term>BinaryData.FromString("\"foo\"")</term>
        /// <description>Creates a payload of "foo".</description>
        /// </item>
        /// <item>
        /// <term>BinaryData.FromObjectAsJson(new { key = "value" })</term>
        /// <description>Creates a payload of { "key": "value" }.</description>
        /// </item>
        /// <item>
        /// <term>BinaryData.FromString("{\"key\": \"value\"}")</term>
        /// <description>Creates a payload of { "key": "value" }.</description>
        /// </item>
        /// </list>
        /// </para>
        /// </summary>
        private IDictionary<string, BinaryData> _serializedAdditionalRawData;

        /// <summary> Initializes a new instance of <see cref="CreateResponse"/>. </summary>
        /// <param name="agent"> The agent to use for generating the response. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="agent"/> is null. </exception>
        public CreateResponse(AgentReference agent)
        {
            Argument.AssertNotNull(agent, nameof(agent));

            Agent = agent;
            Metadata = new ChangeTrackingDictionary<string, string>();
            Tools = new ChangeTrackingList<Tool>();
            Include = new ChangeTrackingList<Includable>();
        }

        /// <summary> Initializes a new instance of <see cref="CreateResponse"/>. </summary>
        /// <param name="agent"> The agent to use for generating the response. </param>
        /// <param name="conversation"> The conversation options to associate with this response. </param>
        /// <param name="metadata">
        /// Set of 16 key-value pairs that can be attached to an object. This can be
        /// useful for storing additional information about the object in a structured
        /// format, and querying for objects via API or the dashboard.
        ///
        /// Keys are strings with a maximum length of 64 characters. Values are strings
        /// with a maximum length of 512 characters.
        /// </param>
        /// <param name="temperature">
        /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
        /// We generally recommend altering this or `top_p` but not both.
        /// </param>
        /// <param name="topP">
        /// An alternative to sampling with temperature, called nucleus sampling,
        /// where the model considers the results of the tokens with top_p probability
        /// mass. So 0.1 means only the tokens comprising the top 10% probability mass
        /// are considered.
        ///
        /// We generally recommend altering this or `temperature` but not both.
        /// </param>
        /// <param name="user"> A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids). </param>
        /// <param name="serviceTier"></param>
        /// <param name="topLogprobs"> An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. </param>
        /// <param name="previousResponseId">
        /// The unique ID of the previous response to the model. Use this to
        /// create multi-turn conversations. Learn more about
        /// [conversation state](/docs/guides/conversation-state).
        /// </param>
        /// <param name="model">
        /// Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI
        /// offers a wide range of models with different capabilities, performance
        /// characteristics, and price points. Refer to the [model guide](/docs/models)
        /// to browse and compare available models.
        /// </param>
        /// <param name="reasoning"></param>
        /// <param name="background">
        /// Whether to run the model response in the background.
        /// [Learn more](/docs/guides/background).
        /// </param>
        /// <param name="maxOutputTokens"> An upper bound for the number of tokens that can be generated for a response, including visible output tokens and [reasoning tokens](/docs/guides/reasoning). </param>
        /// <param name="maxToolCalls"> The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored. </param>
        /// <param name="text">
        /// Configuration options for a text response from the model. Can be plain
        /// text or structured JSON data. Learn more:
        /// - [Text inputs and outputs](/docs/guides/text)
        /// - [Structured Outputs](/docs/guides/structured-outputs)
        /// </param>
        /// <param name="tools">
        /// An array of tools the model may call while generating a response. You
        /// can specify which tool to use by setting the `tool_choice` parameter.
        ///
        /// The two categories of tools you can provide the model are:
        ///
        /// - **Built-in tools**: Tools that are provided by OpenAI that extend the
        ///   model's capabilities, like [web search](/docs/guides/tools-web-search)
        ///   or [file search](/docs/guides/tools-file-search). Learn more about
        ///   [built-in tools](/docs/guides/tools).
        /// - **Function calls (custom tools)**: Functions that are defined by you,
        ///   enabling the model to call your own code. Learn more about
        ///   [function calling](/docs/guides/function-calling).
        /// Please note <see cref="Tool"/> is the base class. According to the scenario, a derived class of the base class might need to be assigned here, or this property needs to be casted to one of the possible derived classes.
        /// The available derived classes include <see cref="CaptureSemanticEventsTool"/>, <see cref="CaptureStructuredOutputsTool"/>, <see cref="CodeInterpreterTool"/>, <see cref="ComputerUsePreviewTool"/>, <see cref="FileSearchTool"/>, <see cref="FunctionTool"/>, <see cref="ImageGenTool"/>, <see cref="LocalShellTool"/>, <see cref="MCPTool"/> and <see cref="WebSearchPreviewTool"/>.
        /// </param>
        /// <param name="toolChoice">
        /// How the model should select which tool (or tools) to use when generating
        /// a response. See the `tools` parameter to see how to specify which tools
        /// the model can call.
        /// </param>
        /// <param name="prompt"></param>
        /// <param name="truncation">
        /// The truncation strategy to use for the model response.
        /// - `auto`: If the context of this response and previous ones exceeds
        ///   the model's context window size, the model will truncate the
        ///   response to fit the context window by dropping input items in the
        ///   middle of the conversation.
        /// - `disabled` (default): If a model response will exceed the context window
        ///   size for a model, the request will fail with a 400 error.
        /// </param>
        /// <param name="input">
        /// Text, image, or file inputs to the model, used to generate a response.
        ///
        /// Learn more:
        /// - [Text inputs and outputs](/docs/guides/text)
        /// - [Image inputs](/docs/guides/images)
        /// - [File inputs](/docs/guides/pdf-files)
        /// - [Conversation state](/docs/guides/conversation-state)
        /// - [Function calling](/docs/guides/function-calling)
        /// </param>
        /// <param name="include">
        /// Specify additional output data to include in the model response. Currently
        /// supported values are:
        /// - `code_interpreter_call.outputs`: Includes the outputs of python code execution
        ///   in code interpreter tool call items.
        /// - `computer_call_output.output.image_url`: Include image urls from the computer call output.
        /// - `file_search_call.results`: Include the search results of
        ///   the file search tool call.
        /// - `message.input_image.image_url`: Include image urls from the input message.
        /// - `message.output_text.logprobs`: Include logprobs with assistant messages.
        /// - `reasoning.encrypted_content`: Includes an encrypted version of reasoning
        ///   tokens in reasoning item outputs. This enables reasoning items to be used in
        ///   multi-turn conversations when using the Responses API statelessly (like
        ///   when the `store` parameter is set to `false`, or when an organization is
        ///   enrolled in the zero data retention program).
        /// </param>
        /// <param name="parallelToolCalls"> Whether to allow the model to run tool calls in parallel. </param>
        /// <param name="store">
        /// Whether to store the generated model response for later retrieval via
        /// API.
        /// </param>
        /// <param name="instructions">
        /// A system (or developer) message inserted into the model's context.
        ///
        /// When using along with `previous_response_id`, the instructions from a previous
        /// response will not be carried over to the next response. This makes it simple
        /// to swap out system (or developer) messages in new responses.
        /// </param>
        /// <param name="stream">
        /// If set to true, the model response data will be streamed to the client
        /// as it is generated using [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).
        /// See the [Streaming section below](/docs/api-reference/responses-streaming)
        /// for more information.
        /// </param>
        /// <param name="serializedAdditionalRawData"> Keeps track of any properties unknown to the library. </param>
        internal CreateResponse(AgentReference agent, ConversationOptions conversation, IDictionary<string, string> metadata, float? temperature, float? topP, string user, ServiceTier? serviceTier, int? topLogprobs, string previousResponseId, ModelIdsResponses? model, Reasoning reasoning, bool? background, int? maxOutputTokens, int? maxToolCalls, CreateResponseText text, IList<Tool> tools, BinaryData toolChoice, Prompt prompt, CreateResponseTruncation? truncation, BinaryData input, IList<Includable> include, bool? parallelToolCalls, bool? store, string instructions, bool? stream, IDictionary<string, BinaryData> serializedAdditionalRawData)
        {
            Agent = agent;
            Conversation = conversation;
            Metadata = metadata;
            Temperature = temperature;
            TopP = topP;
            User = user;
            ServiceTier = serviceTier;
            TopLogprobs = topLogprobs;
            PreviousResponseId = previousResponseId;
            Model = model;
            Reasoning = reasoning;
            Background = background;
            MaxOutputTokens = maxOutputTokens;
            MaxToolCalls = maxToolCalls;
            Text = text;
            Tools = tools;
            ToolChoice = toolChoice;
            Prompt = prompt;
            Truncation = truncation;
            Input = input;
            Include = include;
            ParallelToolCalls = parallelToolCalls;
            Store = store;
            Instructions = instructions;
            Stream = stream;
            _serializedAdditionalRawData = serializedAdditionalRawData;
        }

        /// <summary> Initializes a new instance of <see cref="CreateResponse"/> for deserialization. </summary>
        internal CreateResponse()
        {
        }

        /// <summary> The agent to use for generating the response. </summary>
        public AgentReference Agent { get; }
        /// <summary> The conversation options to associate with this response. </summary>
        public ConversationOptions Conversation { get; set; }
        /// <summary>
        /// Set of 16 key-value pairs that can be attached to an object. This can be
        /// useful for storing additional information about the object in a structured
        /// format, and querying for objects via API or the dashboard.
        ///
        /// Keys are strings with a maximum length of 64 characters. Values are strings
        /// with a maximum length of 512 characters.
        /// </summary>
        public IDictionary<string, string> Metadata { get; }
        /// <summary>
        /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
        /// We generally recommend altering this or `top_p` but not both.
        /// </summary>
        public float? Temperature { get; set; }
        /// <summary>
        /// An alternative to sampling with temperature, called nucleus sampling,
        /// where the model considers the results of the tokens with top_p probability
        /// mass. So 0.1 means only the tokens comprising the top 10% probability mass
        /// are considered.
        ///
        /// We generally recommend altering this or `temperature` but not both.
        /// </summary>
        public float? TopP { get; set; }
        /// <summary> A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids). </summary>
        public string User { get; set; }
        /// <summary> Gets or sets the service tier. </summary>
        public ServiceTier? ServiceTier { get; set; }
        /// <summary> An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. </summary>
        public int? TopLogprobs { get; set; }
        /// <summary>
        /// The unique ID of the previous response to the model. Use this to
        /// create multi-turn conversations. Learn more about
        /// [conversation state](/docs/guides/conversation-state).
        /// </summary>
        public string PreviousResponseId { get; set; }
        /// <summary>
        /// Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI
        /// offers a wide range of models with different capabilities, performance
        /// characteristics, and price points. Refer to the [model guide](/docs/models)
        /// to browse and compare available models.
        /// </summary>
        public ModelIdsResponses? Model { get; set; }
        /// <summary> Gets or sets the reasoning. </summary>
        public Reasoning Reasoning { get; set; }
        /// <summary>
        /// Whether to run the model response in the background.
        /// [Learn more](/docs/guides/background).
        /// </summary>
        public bool? Background { get; set; }
        /// <summary> An upper bound for the number of tokens that can be generated for a response, including visible output tokens and [reasoning tokens](/docs/guides/reasoning). </summary>
        public int? MaxOutputTokens { get; set; }
        /// <summary> The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored. </summary>
        public int? MaxToolCalls { get; set; }
        /// <summary>
        /// Configuration options for a text response from the model. Can be plain
        /// text or structured JSON data. Learn more:
        /// - [Text inputs and outputs](/docs/guides/text)
        /// - [Structured Outputs](/docs/guides/structured-outputs)
        /// </summary>
        public CreateResponseText Text { get; set; }
        /// <summary>
        /// An array of tools the model may call while generating a response. You
        /// can specify which tool to use by setting the `tool_choice` parameter.
        ///
        /// The two categories of tools you can provide the model are:
        ///
        /// - **Built-in tools**: Tools that are provided by OpenAI that extend the
        ///   model's capabilities, like [web search](/docs/guides/tools-web-search)
        ///   or [file search](/docs/guides/tools-file-search). Learn more about
        ///   [built-in tools](/docs/guides/tools).
        /// - **Function calls (custom tools)**: Functions that are defined by you,
        ///   enabling the model to call your own code. Learn more about
        ///   [function calling](/docs/guides/function-calling).
        /// Please note <see cref="Tool"/> is the base class. According to the scenario, a derived class of the base class might need to be assigned here, or this property needs to be casted to one of the possible derived classes.
        /// The available derived classes include <see cref="CaptureSemanticEventsTool"/>, <see cref="CaptureStructuredOutputsTool"/>, <see cref="CodeInterpreterTool"/>, <see cref="ComputerUsePreviewTool"/>, <see cref="FileSearchTool"/>, <see cref="FunctionTool"/>, <see cref="ImageGenTool"/>, <see cref="LocalShellTool"/>, <see cref="MCPTool"/> and <see cref="WebSearchPreviewTool"/>.
        /// </summary>
        public IList<Tool> Tools { get; }
        /// <summary>
        /// How the model should select which tool (or tools) to use when generating
        /// a response. See the `tools` parameter to see how to specify which tools
        /// the model can call.
        /// <para>
        /// To assign an object to this property use <see cref="BinaryData.FromObjectAsJson{T}(T, System.Text.Json.JsonSerializerOptions?)"/>.
        /// </para>
        /// <para>
        /// To assign an already formatted json string to this property use <see cref="BinaryData.FromString(string)"/>.
        /// </para>
        /// <para>
        /// <remarks>
        /// Supported types:
        /// <list type="bullet">
        /// <item>
        /// <description><see cref="ToolChoiceOptions"/></description>
        /// </item>
        /// <item>
        /// <description><see cref="ToolChoiceObject"/></description>
        /// </item>
        /// </list>
        /// </remarks>
        /// Examples:
        /// <list type="bullet">
        /// <item>
        /// <term>BinaryData.FromObjectAsJson("foo")</term>
        /// <description>Creates a payload of "foo".</description>
        /// </item>
        /// <item>
        /// <term>BinaryData.FromString("\"foo\"")</term>
        /// <description>Creates a payload of "foo".</description>
        /// </item>
        /// <item>
        /// <term>BinaryData.FromObjectAsJson(new { key = "value" })</term>
        /// <description>Creates a payload of { "key": "value" }.</description>
        /// </item>
        /// <item>
        /// <term>BinaryData.FromString("{\"key\": \"value\"}")</term>
        /// <description>Creates a payload of { "key": "value" }.</description>
        /// </item>
        /// </list>
        /// </para>
        /// </summary>
        public BinaryData ToolChoice { get; set; }
        /// <summary> Gets or sets the prompt. </summary>
        public Prompt Prompt { get; set; }
        /// <summary>
        /// The truncation strategy to use for the model response.
        /// - `auto`: If the context of this response and previous ones exceeds
        ///   the model's context window size, the model will truncate the
        ///   response to fit the context window by dropping input items in the
        ///   middle of the conversation.
        /// - `disabled` (default): If a model response will exceed the context window
        ///   size for a model, the request will fail with a 400 error.
        /// </summary>
        public CreateResponseTruncation? Truncation { get; set; }
        /// <summary>
        /// Text, image, or file inputs to the model, used to generate a response.
        ///
        /// Learn more:
        /// - [Text inputs and outputs](/docs/guides/text)
        /// - [Image inputs](/docs/guides/images)
        /// - [File inputs](/docs/guides/pdf-files)
        /// - [Conversation state](/docs/guides/conversation-state)
        /// - [Function calling](/docs/guides/function-calling)
        /// <para>
        /// To assign an object to this property use <see cref="BinaryData.FromObjectAsJson{T}(T, System.Text.Json.JsonSerializerOptions?)"/>.
        /// </para>
        /// <para>
        /// To assign an already formatted json string to this property use <see cref="BinaryData.FromString(string)"/>.
        /// </para>
        /// <para>
        /// <remarks>
        /// Supported types:
        /// <list type="bullet">
        /// <item>
        /// <description><see cref="string"/></description>
        /// </item>
        /// <item>
        /// <description><see cref="IList{T}"/> where <c>T</c> is of type <see cref="BinaryData"/></description>
        /// </item>
        /// </list>
        /// </remarks>
        /// Examples:
        /// <list type="bullet">
        /// <item>
        /// <term>BinaryData.FromObjectAsJson("foo")</term>
        /// <description>Creates a payload of "foo".</description>
        /// </item>
        /// <item>
        /// <term>BinaryData.FromString("\"foo\"")</term>
        /// <description>Creates a payload of "foo".</description>
        /// </item>
        /// <item>
        /// <term>BinaryData.FromObjectAsJson(new { key = "value" })</term>
        /// <description>Creates a payload of { "key": "value" }.</description>
        /// </item>
        /// <item>
        /// <term>BinaryData.FromString("{\"key\": \"value\"}")</term>
        /// <description>Creates a payload of { "key": "value" }.</description>
        /// </item>
        /// </list>
        /// </para>
        /// </summary>
        public BinaryData Input { get; set; }
        /// <summary>
        /// Specify additional output data to include in the model response. Currently
        /// supported values are:
        /// - `code_interpreter_call.outputs`: Includes the outputs of python code execution
        ///   in code interpreter tool call items.
        /// - `computer_call_output.output.image_url`: Include image urls from the computer call output.
        /// - `file_search_call.results`: Include the search results of
        ///   the file search tool call.
        /// - `message.input_image.image_url`: Include image urls from the input message.
        /// - `message.output_text.logprobs`: Include logprobs with assistant messages.
        /// - `reasoning.encrypted_content`: Includes an encrypted version of reasoning
        ///   tokens in reasoning item outputs. This enables reasoning items to be used in
        ///   multi-turn conversations when using the Responses API statelessly (like
        ///   when the `store` parameter is set to `false`, or when an organization is
        ///   enrolled in the zero data retention program).
        /// </summary>
        public IList<Includable> Include { get; set; }
        /// <summary> Whether to allow the model to run tool calls in parallel. </summary>
        public bool? ParallelToolCalls { get; set; }
        /// <summary>
        /// Whether to store the generated model response for later retrieval via
        /// API.
        /// </summary>
        public bool? Store { get; set; }
        /// <summary>
        /// A system (or developer) message inserted into the model's context.
        ///
        /// When using along with `previous_response_id`, the instructions from a previous
        /// response will not be carried over to the next response. This makes it simple
        /// to swap out system (or developer) messages in new responses.
        /// </summary>
        public string Instructions { get; set; }
        /// <summary>
        /// If set to true, the model response data will be streamed to the client
        /// as it is generated using [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).
        /// See the [Streaming section below](/docs/api-reference/responses-streaming)
        /// for more information.
        /// </summary>
        public bool? Stream { get; set; }
    }
}
